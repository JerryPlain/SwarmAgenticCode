"""
PSO (Particle Swarm Optimization) for MGSM Task
"""

from langchain_openai import ChatOpenAI

import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

import argparse
import copy
import json
import os
import random
from tqdm import tqdm
from func import *
from role import Team
from eval import evaluate, get_fitness
from logger import setup_logger, log, log_all

from prompt.team_update import update_team
from prompt.velocity_init import initialize_velocity
from prompt.velocity_update import update_velocity
from prompt.failure_identify import identify_failure
from prompt.failure_improve import improve_failure
from prompt.best_global import reflect_from_global_best
from prompt.best_personal import reflect_from_personal_best
from prompt.feedback_give import give_feedback
from prompt.feedback_summarize import summarize_feedback


class Particle:
    def __init__(self, position, logger, llm, save_dir='results', max_workers=None):
        """initializes the particle with the position and logger. Position is encoded as a tuple of team and code.

        Args:
            position (tuple): (team, code)
            logger (logger): a logger object
            llm: LLM model to use for evaluation and updates
            save_dir (str): directory to save results (default: 'results')
            max_workers (int): maximum number of worker threads (default: None, uses system default)
        """
        self.position = position
        self.velocity = None
        self.fitness = 0.0
        # detected flaws
        self.evaluation = None

        self.best_position = None
        self.best_fitness = 0.0  

        self.logger = logger
        self.llm = llm
        self.save_dir = save_dir
        self.max_workers = max_workers
        self.fitness_history = []  

    async def evaluate(self, dataset, iter, i_pos):
        """1. Execute the code generated and evaluate the response and score (internal execute function), and record the coherence problems and logs.
        2. Execute the process for each data in the dataset, also save the results in a jsonl file.
        3. Update the personal best fitness and perform flaw detection.
        Args:
            dataset (_type_): dataset to be used for evaluation
            iter (int): index of the iteration
            i_pos (int): index of the particle

        Returns:
            _type_: _description_
        """
        team = self.position[0]
        code = self.position[1]
        func = set_forward(code)

        def execute(team_with_task, data, evaluations, i, batch_logs):
            """1. Executes the workflow function of the team with the data from the dataset.
            2. Evaluates the response generated by the team with the LLM model. score and problem of the response is recorded.
            3. If there is a problem, feedback is generated and added to the evaluations list.
            4. Logs are generated and added to the batch_logs list.


            Args:
                team_with_task (team): team object
                data (_type_): example['inputs'](input for llm) and example['targets'](answers)
                evaluations (list): list of evaluations, add evaluations to this list
                i (int): i-th particle
                batch_logs (list): list of logs

            Returns:
                dict: result is a dictionary with response and score. response is generated by the workflow function of the team. score is the evaluated by LLM.
            """
            logs = [(f'Iter {iter} - Data {i}', '\n# Roles Message', None)]
            
            task_instance = data['inputs']
            team_with_task.reset_task(task_instance)
            times = 0
            while times < 3:
                try:
                    res = func(team_with_task)
                    logs.extend(team_with_task.logs)
                    break
                
                except Exception as e:
                    logs = [(f'Iter {iter} - Data {i}', '\n# Roles Message', None)]
                    team_with_task.reset_task(task_instance)
                    print(f'{e}')
                    times += 1

            score, problem = evaluate(self.llm, res, data['targets'])
            logs.append((f'Iter {iter} - Data {i} - Correct', f'''{score}\n{data['targets']}''', None))
            
            if problem != '':
                evaluation = give_feedback(self.llm, self.logger, task_instance, team_with_task.patch_result_and_workflow(), problem)
                evaluations.append(evaluation)
            batch_logs.append(logs)
            
            result = {
                    "response": res,
                    "score": score,
            }
            return result

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            loop = asyncio.get_running_loop()
            tasks = []
            evaluations = []
            batch_logs = []
            for i, data in enumerate(dataset):
                team_with_task = team.deepcopy() 
                tasks.append(loop.run_in_executor(executor, execute, team_with_task, data, evaluations, i, batch_logs))

            results = await asyncio.gather(*tasks)
            for logs in batch_logs:
                log_all(self.logger, logs)

        # Create save directory if it doesn't exist
        os.makedirs(self.save_dir, exist_ok=True)
        write_jsonl(f'{self.save_dir}/results-{iter}-{i_pos}.jsonl', results, 'w')

        # update personal best fitness and perform flaw detection
        self.fitness = get_fitness(results)
        log(self.logger, 'Pass Rate', self.fitness)
        
        self.fitness_history.append(self.fitness)               
        if self.fitness >= self.best_fitness:
            self.best_position = (self.position[0].save_into_dict(), self.position[1])
            self.best_fitness = self.fitness

        evaluations = '\n'.join(f"**Feedback {i+1}:**\n{item}\n" for i, item in enumerate(evaluations))
        self.evaluation = summarize_feedback(self.llm, self.logger, evaluations, json.dumps(team.save_into_dict(), indent=4))
 
    def update_velocity(self, global_best_position):
        team = json.dumps(self.position[0].save_into_dict(), indent=4)
        global_best_team = json.dumps(global_best_position[0], indent=4)
        personal_best_team = json.dumps(self.best_position[0], indent=4)
        
        evaluation = self.evaluation

        if team == personal_best_team:
            p_best = None
        else:
            p_best = reflect_from_personal_best(self.llm, self.logger, team, evaluation, personal_best_team)
            
        if team == global_best_team:
            g_best = None
        else:
            g_best = reflect_from_global_best(self.llm, self.logger, team, evaluation, global_best_team)
        
        if self.velocity is None:
            velocity = initialize_velocity(self.llm, self.logger, team, evaluation)
            if team == personal_best_team and team == global_best_team:
                self.velocity = velocity
            else:
                self.velocity = update_velocity(self.llm, self.logger, team, velocity, g_best, p_best)
        else:
            # check if previous velocity has failed adjustments
            failures = identify_failure(self.llm, self.logger, evaluation, self.velocity)
            velocity = improve_failure(self.llm, self.logger, team, failures)
            # if the team is the best team, the failed adjustment is removed
            if team == personal_best_team and team == global_best_team:
                clean_velocity = [{k: v for k, v in item.items() if k != "Failed Adjustment"} for item in velocity]
                self.velocity = clean_velocity
            else:
                self.velocity = update_velocity(self.llm, self.logger, team, velocity, g_best, p_best)

    def update_position(self):
        team = self.position[0]
        new_team = update_team(self.llm, self.logger, team.to_str(), team.workflow, self.velocity)
        team.update(new_team)
        new_code = get_forward(self.llm, self.logger, team.to_str(), team.workflow)
        self.position = (team, new_code)


def initialize(settings, llm_role, llm_eval, model, save_dir='results', max_workers=None):
    """initialize the particles with the settings and llm. A team is created for the task. 

    Args:
        settings (list): list of temperature settings for the particles
        llm_role: LLM model to be used for role initialization
        llm_eval: LLM model to be used for evaluation
        model (str): model name for team initialization
        save_dir (str): directory to save results
        max_workers (int): maximum number of worker threads

    Returns:
        list: list of particles
    """
    particles = []
    for i, item in enumerate(settings): 
        logger = setup_logger(i)
        llm = ChatOpenAI(model=model, temperature=item)
        team = Team(llm=llm_role, logger=logger)
        team.init(llm=llm)
        code = get_forward(llm_eval, logger, team.to_str(), team.workflow) 
        particle = Particle(
            position = (team, code),
            logger = logger,
            llm = llm_eval,
            save_dir = save_dir,
            max_workers = max_workers
        )
        particles.append(particle)
    return particles


def initialize_with_archive(idx: int, llm_role, llm_eval, save_dir='results', max_workers=None):
    """Initialize particles from archived results.
    
    Args:
        idx (int): index of the archive to load
        llm_role: LLM model to be used for role initialization
        llm_eval: LLM model to be used for evaluation
        save_dir (str): directory to save results
        max_workers (int): maximum number of worker threads
        
    Returns:
        list: list of particles
    """
    particles = []
    archives = load_particles(idx)
    for i, archive in enumerate(archives): 
        logger = setup_logger(i)
        team = Team(llm=llm_role, logger=logger)
        team.update(archive['team'])
        code = archive['code'] 
        particle = Particle(
            position = (team, code),
            logger = logger,
            llm = llm_eval,
            save_dir = save_dir,
            max_workers = max_workers
        )
        particles.append(particle)
    return particles


def update_global_best(particles, global_best_position, global_best_fitness):
    """Update global best position and fitness based on particle evaluations.
    
    Args:
        particles (list): list of particles
        global_best_position: current global best position
        global_best_fitness (float): current global best fitness
        
    Returns:
        tuple: (updated global best position, updated global best fitness)
    """
    g_best_position = global_best_position
    g_best_fitness = global_best_fitness
    
    for p in particles:
        if p.fitness >= g_best_fitness:
            g_best_position = p.best_position
            g_best_fitness = p.fitness
    
    return g_best_position, g_best_fitness


async def main(max_iteration=5, settings=None, model='gpt-4o-mini', max_workers=None, save_dir='results', 
               dataset_size=128, dataset_start=0, random_seed=0):
    """Main function to run PSO optimization.
    
    Args:
        max_iteration (int): Maximum number of iterations (default: 5)
        settings (list): Temperature settings for each particle (default: [0.2, 0.6, 1.0])
        model (str): Model name to use (default: 'gpt-4o-mini')
        max_workers (int): Maximum number of worker threads (default: None, uses system default)
        save_dir (str): Directory to save results (default: 'results')
        dataset_size (int): Number of examples to use from dataset (default: 128)
        dataset_start (int): Starting index for dataset selection (default: 0)
        random_seed (int): Random seed for dataset shuffling (default: 0)
    """
    # Load dataset
    datasets = get_all_examples()
    random.seed(random_seed)
    random.shuffle(datasets)
    
    # Select dataset slice
    end_idx = dataset_start + dataset_size
    datasets = datasets[dataset_start:end_idx]
    print(f"Using {len(datasets)} examples from dataset (indices {dataset_start}:{end_idx})")
    
    # Save dataset for reference
    os.makedirs(save_dir, exist_ok=True)
    write_jsonl(f"{save_dir}/dataset.jsonl", datasets, 'w')

    iter = 0
    # temperature of each particle
    if settings is None:
        settings = [0.2, 0.6, 1.0]

    particles = []
    global_best_position = None
    global_best_fitness = 0.0
    global_best_trend = []

    # Hyper Parameter
    llm_role = ChatOpenAI(model=model, temperature=0.001)
    llm_eval = ChatOpenAI(model=model, temperature=0.001)

    particles = initialize(settings, llm_role, llm_eval, model, save_dir, max_workers)

    for iter in tqdm(range(max_iteration), desc="Iteration", position=0):
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # evaluate particles and update personal best
            evaluate_tasks = []
            for i, p in enumerate(particles):
                evaluate_tasks.append(p.evaluate(datasets, iter, i))
            awaitables = asyncio.as_completed(evaluate_tasks)
            for _ in tqdm(awaitables, desc="Particles Evaluate", total=len(particles), position=1):
                await _
            # update global best
            global_best_position, global_best_fitness = update_global_best(particles, global_best_position, global_best_fitness)
            global_best_trend.append(copy.deepcopy(global_best_fitness))

            if iter < max_iteration - 1:
                velocity_futures = []
                for p in particles:
                    velocity_futures.append(executor.submit(p.update_velocity, global_best_position))
                for future in tqdm(as_completed(velocity_futures), desc="Update Velocity", total=len(particles), position=2):
                    future.result()

                position_futures = []
                for p in particles:
                    position_futures.append(executor.submit(p.update_position))
                for future in tqdm(as_completed(position_futures), desc="Update Position", total=len(particles), position=2):
                    future.result()

    save_particles(particles)
    
    print(f"\nOptimization completed!")
    print(f"Global best fitness: {global_best_fitness}")
    print(f"Global best trend: {global_best_trend}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='PSO Optimization for MGSM Task')
    parser.add_argument('--max_iteration', type=int, default=10, 
                        help='Maximum number of iterations (default: 10)')
    parser.add_argument('--settings', type=float, nargs='+', default=[0.2, 0.4, 0.6, 0.8, 1],
                        help='Temperature settings for each particle (default: [0.2, 0.4, 0.6, 0.8, 1]). Example: --settings 0.2 0.4 0.6 0.8 1')
    parser.add_argument('--model', type=str, default='gpt-4o-mini',
                        choices=['gpt-4-turbo-2024-04-09', 'gpt-3.5-turbo-0125', 'gpt-4o-2024-05-13', 'gpt-4o-mini', 'gpt-4o'],
                        help='Model name to use (default: gpt-4o-mini)')
    parser.add_argument('--max_workers', type=int, default=None,
                        help='Maximum number of worker threads (default: None, uses system default)')
    parser.add_argument('--save_dir', type=str, default='results',
                        help='Directory to save results (default: results)')
    parser.add_argument('--dataset_size', type=int, default=128,
                        help='Number of examples to use from dataset (default: 128)')
    parser.add_argument('--dataset_start', type=int, default=0,
                        help='Starting index for dataset selection (default: 0)')
    parser.add_argument('--random_seed', type=int, default=0,
                        help='Random seed for dataset shuffling (default: 0)')
    
    args = parser.parse_args()
    
    asyncio.run(main(
        max_iteration=args.max_iteration, 
        settings=args.settings, 
        model=args.model,
        max_workers=args.max_workers,
        save_dir=args.save_dir,
        dataset_size=args.dataset_size,
        dataset_start=args.dataset_start,
        random_seed=args.random_seed
    ))

